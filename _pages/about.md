---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
{% include base_path %}
Hello! My name is Zachary Novack, and I am currently a 3rd Year Computer Science PhD Student at UC San Diego, where I am advised by <a href="https://cseweb.ucsd.edu/~jmcauley/" target="_blank">Prof. Julian McAuley</a> and <a href="https://cseweb.ucsd.edu/~tberg/" target="_blank">Prof. Taylor Berg-Kirkpatrick</a>. Previously, I studied statistics and machine learning at Carnegie Mellon University, and was primarily advised by <a href="https://www.zacharylipton.com/" target="_blank">Prof. Zachary Lipton</a> and <a href="https://sites.santafe.edu/~simon/" target="_blank">Prof. Simon DeDeo</a>. 

As someone passionate about building **Generative Music and Audio** systems, my research primarily focuses on three pillars od bringing the state of the art in music generation to practical usability: **Controllability**, **Efficiency**, and **Interactivity**. Specifically, I am interested in investigating <a href="https://ditto-music.github.io/web/" target="_blank">*training-free* control for generative music models</a>  [(ICML 2024)](https://arxiv.org/abs/2401.12179) and <a href="https://ditto-music.github.io/ditto2/" target="_blank">accelerating generative systems to *faster than real time* speeds </a>  [(ISMIR 2024)](https://arxiv.org/abs/2405.20289). Additionally, a high level goal of mine is to leverage AI systems to help *real musicians* <a href="https://arxiv.org/abs/2310.10772" target="_blank">**understand**</a> [(AES 2024)](https://arxiv.org/abs/2310.10772), **learn**, and **perform** music more effectively.
<!-- Specifically, I have been recently interested in controllable systems for music learning, predicting student classroom performance, and widely making deep learning an empirically-motivated research practice.  -->

In the past, I've worked on general multi-modal reasoning tasks [(ICML 2023)](https://arxiv.org/abs/2302.02551) and empirical deep optimization theory [(ICLR 2023)](https://arxiv.org/abs/2211.15853).

<!-- In the past, I have also worked extensively in computational social science, chiefly within the realms of linguistic bias and social media usage. -->

In my free time, I enjoy making [experimental computer music](https://zacharynovack.github.io/music/), and teach the front ensemble at 10-time world championship finalist [POW Percussion Ensemble](https://pulsepercussion.org/pow)!

<hr>

<h3 id="updates"><strong>Updates</strong></h3>
<style> table, tr, td { border: none; }</style>
<div style="height:250px;overflow:auto;border:0px;border-collapse: collapse;">
<table border="none" style="border:0px;border-collapse: collapse;" rules="none">
<colgroup><col span="1" style="width: 12%;"><col span="1" style="width: 88%;"></colgroup><tbody><tr><td>
<b> June 2024:</b></td><td> Our work on <a href="https://ditto-music.github.io/ditto2/">accelerated training-free editing and control for text-to-music diffusion models</a> is accepted at ISMIR 2024 in San Francisco!</td></tr><tr><td> 
<b> May 2024:</b></td><td> Our work on <a href="https://ditto-music.github.io/web/">training-free editing and control for text-to-music diffusion models</a> is accepted at ICML 2024 in Vienna as an ORAL, and our work on <a href="https://arxiv.org/abs/2302.02551">unsupervised lead sheet generation</a> is accepted at the AES Symposium for AI and the Musician in Boston!</td></tr><tr><td> 
<b> January 2024:</b></td><td> Our work on <a href="https://ditto-music.github.io/web/">training-free editing and control for text-to-music diffusion models</a> is out on arxiv!</td></tr><tr><td> 
<b> October 2023:</b></td><td> Our work on <a href="https://arxiv.org/abs/2302.02551">unsupervised lead sheet generation</a> is out on arxiv!</td></tr><tr><td> 
<b> June 2023:</b></td><td> Started Research Scientist internship with Nicholas Bryan at the Adobe Research Audio Group!</td></tr><tr><td> 
<b> April 2023:</b></td><td> Our work on <a href="https://arxiv.org/abs/2302.02551"> augmenting CLIP zero-shot inference with hierarchical label sets</a> was accepted to ICML 2023 in Honolulu, Hawaii!</td></tr><tr><td> 
<b> March 2023:</b></td><td> Our work on <a href="https://arxiv.org/abs/2302.02551"> augmenting CLIP zero-shot inference with hierarchical label sets</a> was accepted to the ICLR 2023 1st Workshop on Multimodal Representation Learning!</td></tr><tr><td> 
<b> January 2023:</b></td><td> Our work on <a href="https://arxiv.org/abs/2211.15853"> understanding implicit regularization mechanisms in SGD</a> was accepted to ICLR 2023 in Kigali, Rwanda!</td></tr><tr><td> 
<b> December 2022:</b></td><td> Our work on <a href="https://arxiv.org/abs/2211.15853"> understanding implicit regularization mechanisms in SGD</a> got accepted to the NeurIPS 2022 Workshop on the Benefits of Higher Order Optimization in Machine Learning (HOO-ML), as a Spotlight and won Best Poster!</td></tr><tr><td>
<b> September 2022:</b></td><td> Began CS PhD at UCSD!</td></tr><tr><td>
<b> May 2022:</b></td><td> Submitted senior thesis on modeling social media addiction on Twitter to <a href="https://kilthub.cmu.edu/articles/thesis/Down_the_Rabbit_Hole_Modeling_Twitter_Dynamics_through_Bayesian_Inference/20638989" >CMU Kilthub</a>. </td></tr><tr><td>
<b> May 2022:</b></td><td> Graduated from CMU with B.S. in Statistics & Machine Learning, and a minor in Sonic Arts!</td></tr></tbody></table></div>