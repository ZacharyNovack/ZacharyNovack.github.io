---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
{% include base_path %}
Hello! My name is Zachary Novack, and I am currently a Computer Science PhD Candidate at UC San Diego, where I am advised by <a href="https://cseweb.ucsd.edu/~jmcauley/" target="_blank">Prof. Julian McAuley</a> and <a href="https://cseweb.ucsd.edu/~tberg/" target="_blank">Prof. Taylor Berg-Kirkpatrick</a>. Previously, I studied statistics and machine learning at Carnegie Mellon University, and was primarily advised by <a href="https://www.zacharylipton.com/" target="_blank">Prof. Zachary Lipton</a> and <a href="https://sites.santafe.edu/~simon/" target="_blank">Prof. Simon DeDeo</a>. 

**I am actively on the job market and looking for industry, academic, and postdoc positions starting fall 2026!**


As someone passionate about building **Interactive Generative Audio/Music** systems, my research primarily focuses on two main pillars of bringing the state of the art in audio generation to practical usability: **Controllability** and **Efficiency**. In particular, I have worked on extending generative audio systems with <a href="https://ditto-music.github.io/web/" target="_blank">*training-free*</a>  [(ICML 2024, Oral)](https://arxiv.org/abs/2401.12179) and <a href="https://havenpersona.github.io/ossl-v1/" target="_blank">*multimodal*</a> [(ISMIR 2025)](https://arxiv.org/abs/2506.12573) controls, how to accelerate both <a href="https://presto-music.github.io/web/" target="_blank">*music*</a> [(ICLR 2025, Spotlight)](https://arxiv.org/abs/2410.05167) and <a href="https://arc-text2audio.github.io/web/" target="_blank">*general audio*</a> [(WASPAA 2025)](https://arxiv.org/abs/2505.08175) generation models, and how to make such <a href="https://ditto-music.github.io/ditto2/" target="_blank">interactive controls faster </a>  [(ISMIR 2024)](https://arxiv.org/abs/2405.20289).

 Additionally, I've also collaborated extensively on researching **multi-modal text-audio reasoning**, both on the modeling side, working on improving long-form audio retrieval [(ICASSP 2025, Oral)](https://arxiv.org/abs/2410.02271), as well as designing better evaluation benchmarks to accurately measure audio perception [(ISMIR 2025, Best Paper Award)](https://arxiv.org/abs/2504.00369).
<!-- Specifically, I have been recently interested in controllable systems for music learning, predicting student classroom performance, and widely making deep learning an empirically-motivated research practice.  -->

In the past, I've worked on general multi-modal reasoning tasks [(ICML 2023)](https://arxiv.org/abs/2302.02551) and empirical deep optimization theory [(ICLR 2023)](https://arxiv.org/abs/2211.15853).

<!-- In the past, I have also worked extensively in computational social science, chiefly within the realms of linguistic bias and social media usage. -->

In my free time, I enjoy cooking, playing beach volleyball (a must in San Diego!), as well as teaching the front ensemble at 11-time world championship finalist [POW Percussion Ensemble](https://pulsepercussion.org/pow)!

<hr>

<h3 id="updates"><strong>Updates</strong></h3>
<style> table, tr, td { border: none; }</style>
<div style="height:250px;overflow:auto;border:0px;border-collapse: collapse;">
<table border="none" style="border:0px;border-collapse: collapse;" rules="none">
<colgroup><col span="1" style="width: 12%;"><col span="1" style="width: 88%;"></colgroup><tbody><tr><td>
<b> September 2025:</b></td><td> <a href="https://arxiv.org/abs/2504.00369">Are you really listening?</a> won best paper at ISMIR!</td></tr><tr><td> 
<b> September 2025:</b></td><td> 1 paper in at NeurIPS as a spotlight!</td></tr><tr><td> 
<b> July 2025:</b></td><td> 1 paper in at WASPAA!</td></tr><tr><td> 
<b> June 2025:</b></td><td> 3 papers in at ISMIR!</td></tr><tr><td> 
<b> May 2025:</b></td><td> I officially proposed my thesis and became a PhD Candidate!</td></tr><tr><td> 
<b> May 2025:</b></td><td> Taught four lectures on audio-domain music generation in our new <a href="https://cseweb.ucsd.edu/classes/sp25/cse253-a/">AI Music Course, CSE153/253</a> at UCSD.</td></tr><tr><td> 
<b> January 2025:</b></td><td> Our work on <a href="https://presto-music.github.io/web/">accelerating text-to-music diffusion models</a> is accepted at ICLR 2025 in Singapore as a Spotlight!</td></tr><tr><td> 
<b> December 2024:</b></td><td> Three works from the UCSD MUSAIC Group accepted at ICASSP 2025!</td></tr><tr><td> 
<b> October 2024:</b></td><td> Our work on <a href="https://presto-music.github.io/web/">accelerating text-to-music diffusion models</a> is out on arXiv!</td></tr><tr><td> 
<b> October 2024:</b></td><td> Our work on <a href="https://arxiv.org/abs/2410.02271">long-form text-audio contrastive learning</a> is out on arXiv!</td></tr><tr><td> 
<b> September 2024:</b></td><td> Our work on <a href="https://pnlong.github.io/PDMX.demo/">the largest dataset of public domain of symbolic music scores</a> is out on arXiv!</td></tr><tr><td> 
<b> June 2024:</b></td><td> Our work on <a href="https://ditto-music.github.io/ditto2/">accelerated training-free editing and control for text-to-music diffusion models</a> is accepted at ISMIR 2024 in San Francisco!</td></tr><tr><td> 
<b> May 2024:</b></td><td> Our work on <a href="https://ditto-music.github.io/web/">training-free editing and control for text-to-music diffusion models</a> is accepted at ICML 2024 in Vienna as an ORAL, and our work on <a href="https://arxiv.org/abs/2302.02551">unsupervised lead sheet generation</a> is accepted at the AES Symposium for AI and the Musician in Boston!</td></tr><tr><td> 
<b> January 2024:</b></td><td> Our work on <a href="https://ditto-music.github.io/web/">training-free editing and control for text-to-music diffusion models</a> is out on arxiv!</td></tr><tr><td> 
<b> October 2023:</b></td><td> Our work on <a href="https://arxiv.org/abs/2302.02551">unsupervised lead sheet generation</a> is out on arxiv!</td></tr><tr><td> 
<b> June 2023:</b></td><td> Started Research Scientist internship with Nicholas Bryan at the Adobe Research Audio Group!</td></tr><tr><td> 
<b> April 2023:</b></td><td> Our work on <a href="https://arxiv.org/abs/2302.02551"> augmenting CLIP zero-shot inference with hierarchical label sets</a> was accepted to ICML 2023 in Honolulu, Hawaii!</td></tr><tr><td> 
<b> March 2023:</b></td><td> Our work on <a href="https://arxiv.org/abs/2302.02551"> augmenting CLIP zero-shot inference with hierarchical label sets</a> was accepted to the ICLR 2023 1st Workshop on Multimodal Representation Learning!</td></tr><tr><td> 
<b> January 2023:</b></td><td> Our work on <a href="https://arxiv.org/abs/2211.15853"> understanding implicit regularization mechanisms in SGD</a> was accepted to ICLR 2023 in Kigali, Rwanda!</td></tr><tr><td> 
<b> December 2022:</b></td><td> Our work on <a href="https://arxiv.org/abs/2211.15853"> understanding implicit regularization mechanisms in SGD</a> got accepted to the NeurIPS 2022 Workshop on the Benefits of Higher Order Optimization in Machine Learning (HOO-ML), as a Spotlight and won Best Poster!</td></tr><tr><td>
<b> September 2022:</b></td><td> Began CS PhD at UCSD!</td></tr><tr><td>
<b> May 2022:</b></td><td> Submitted senior thesis on modeling social media addiction on Twitter to <a href="https://kilthub.cmu.edu/articles/thesis/Down_the_Rabbit_Hole_Modeling_Twitter_Dynamics_through_Bayesian_Inference/20638989" >CMU Kilthub</a>. </td></tr><tr><td>
<b> May 2022:</b></td><td> Graduated from CMU with B.S. in Statistics & Machine Learning, and a minor in Sonic Arts!</td></tr></tbody></table></div>